name: Test Suite

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: '20.16.0'
  PYTHON_VERSION: '3.11'
  PNPM_VERSION: '10.12.4'

jobs:
  # Client Tests
  client-tests:
    name: Client Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    defaults:
      run:
        working-directory: ./client2

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: ./.github/actions/setup-pnpm-corepack
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'pnpm'
          cache-dependency-path: 'pnpm-lock.yaml'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Lint code
        run: pnpm run lint

      - name: Type check
        run: pnpm run typecheck

      - name: Run unit tests with coverage
        run: pnpm run test:coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          flags: client
          directory: ./client2/coverage
          fail_ci_if_error: true
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: client-test-results
          path: |
            ./client2/coverage/
            ./client2/test-results/
          retention-days: 7

  # Server Tests
  server-tests:
    name: Server Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    defaults:
      run:
        working-directory: ./server2

    services:
      # Mock Supabase service for testing
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: ./.github/actions/setup-pnpm-corepack
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'pnpm'
          cache-dependency-path: 'pnpm-lock.yaml'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Setup test environment
        run: |
          cp .env.test .env
          echo "DATABASE_URL=postgresql://postgres:postgres@localhost:5432/test_db" >> .env

      - name: Run tests with coverage
        run: pnpm run test:coverage
        env:
          NODE_ENV: test

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          flags: server
          directory: ./server2/coverage
          fail_ci_if_error: true
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: server-test-results
          path: |
            ./server2/coverage/
            ./server2/test-results/
          retention-days: 7

  # AI Service Tests
  ai-service-tests:
    name: AI Service Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    defaults:
      run:
        working-directory: ./ai-service

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt

      - name: Setup test environment
        run: cp .env.test .env

      - name: Run tests with coverage
        run: |
          pytest --cov=src --cov-report=xml --cov-report=html --cov-report=term-missing
        env:
          ENVIRONMENT: test
          GOOGLE_API_KEY: test-api-key

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          flags: ai-service
          directory: ./ai-service
          fail_ci_if_error: true
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ai-service-test-results
          path: |
            ./ai-service/htmlcov/
            ./ai-service/coverage.xml
            ./ai-service/.pytest_cache/
          retention-days: 7

  # E2E Tests
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [client-tests, server-tests, ai-service-tests]

    defaults:
      run:
        working-directory: ./client2

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: ./.github/actions/setup-pnpm-corepack
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'pnpm'
          cache-dependency-path: './client2/pnpm-lock.yaml'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install client dependencies
        run: pnpm install --frozen-lockfile

      - name: Install server dependencies
        run: |
          cd ../server2
          pnpm install --frozen-lockfile

      - name: Install AI service dependencies
        run: |
          cd ../ai-service
          pip install -r requirements.txt

      - name: Setup test environments
        run: |
          cp .env.test .env
          cd ../server2 && cp .env.test .env
          cd ../ai-service && cp .env.test .env

      - name: Install Playwright browsers
        run: pnpm exec playwright install --with-deps

      - name: Build client
        run: pnpm run build

      - name: Start services in background
        run: |
          cd ../server2 && pnpm run start &
          cd ../ai-service && python -m uvicorn main:app --port 8000 &
          sleep 10  # Wait for services to start

      - name: Run E2E tests
        run: pnpm run test:e2e
        env:
          PLAYWRIGHT_TEST_BASE_URL: http://localhost:3000

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            ./client2/test-results/
            ./client2/playwright-report/
          retention-days: 7

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Setup Node.js for dependency check
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'

      - name: Setup pnpm
        uses: ./.github/actions/setup-pnpm-corepack
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Check for vulnerabilities in client
        run: |
          cd client2
          pnpm audit --audit-level moderate

      - name: Check for vulnerabilities in server
        run: |
          cd server2
          pnpm audit --audit-level moderate

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup pnpm
        uses: ./.github/actions/setup-pnpm-corepack
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install client dependencies
        run: |
          cd client2
          pnpm install --frozen-lockfile

      - name: Build client for performance testing
        run: |
          cd client2
          pnpm run build

      - name: Run Lighthouse CI
        uses: treosh/lighthouse-ci-action@v10
        with:
          configPath: './client2/.lighthouserc.json'
          uploadArtifacts: true
          temporaryPublicStorage: true

  # Test Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [client-tests, server-tests, ai-service-tests, e2e-tests, security-tests]
    if: always()

    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v4

      - name: Generate test summary
        run: |
          echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Status Overview" >> $GITHUB_STEP_SUMMARY
          echo "- Client Tests: ${{ needs.client-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Server Tests: ${{ needs.server-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- AI Service Tests: ${{ needs.ai-service-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- E2E Tests: ${{ needs.e2e-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Security Tests: ${{ needs.security-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Coverage Reports" >> $GITHUB_STEP_SUMMARY
          echo "Coverage reports have been uploaded to Codecov." >> $GITHUB_STEP_SUMMARY

      - name: Report test status
        if: ${{ needs.client-tests.result == 'failure' || needs.server-tests.result == 'failure' || needs.ai-service-tests.result == 'failure' || needs.e2e-tests.result == 'failure' }}
        run: |
          echo "❌ Some tests failed. Please check the logs above."
          exit 1
