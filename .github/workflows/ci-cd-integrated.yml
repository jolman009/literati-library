# Integrated CI/CD Pipeline - Production Ready
# Combines testing, building, and deployment with the new environment system

name: Production Ready CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      skip_tests:
        description: 'Skip tests (not recommended)'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '20.x'
  PYTHON_VERSION: '3.11'
  PNPM_VERSION: 'latest'

jobs:
  # Quality Gates - All tests and checks
  quality-gates:
    name: Quality Gates & Testing
    runs-on: ubuntu-latest
    outputs:
      should-deploy: ${{ steps.deploy-decision.outputs.should-deploy }}
      target-env: ${{ steps.deploy-decision.outputs.target-env }}

    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: 🔧 Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
          cache-dependency-path: 'client2/pnpm-lock.yaml'

      # Client Testing & Security
      - name: 📦 Install Client Dependencies
        working-directory: ./client2
        run: pnpm install --frozen-lockfile

      - name: 🔍 Client Lint & Type Check
        working-directory: ./client2
        run: |
          pnpm run lint || echo "⚠️ Linting issues found"
          pnpm run typecheck || echo "⚠️ Type check issues found"
        continue-on-error: true

      - name: 🧪 Client Unit Tests
        working-directory: ./client2
        run: pnpm run test:coverage || echo "⚠️ Some tests failed"
        env:
          CI: true
        continue-on-error: ${{ github.event.inputs.skip_tests == 'true' }}

      - name: 🔒 Client Security Audit
        working-directory: ./client2
        run: |
          pnpm audit --audit-level moderate || echo "⚠️ Security vulnerabilities found"
        continue-on-error: true

      # Server Testing & Security
      - name: 📦 Install Server Dependencies
        working-directory: ./server2
        run: pnpm install --frozen-lockfile

      - name: 🧪 Server Unit Tests
        working-directory: ./server2
        run: pnpm run test:coverage || echo "⚠️ Some server tests failed"
        env:
          CI: true
          NODE_ENV: test
          JWT_SECRET: test_jwt_secret_32_characters_long
        continue-on-error: ${{ github.event.inputs.skip_tests == 'true' }}

      - name: 🔒 Server Security Audit
        working-directory: ./server2
        run: |
          pnpm audit --audit-level moderate || echo "⚠️ Security vulnerabilities found"
        continue-on-error: true

      # Build Testing
      - name: 🏗️ Test Build - Staging
        working-directory: ./client2
        env:
          VITE_API_BASE_URL: https://literati-api-staging.onrender.com
          VITE_AI_SERVICE_URL: https://literati-ai-staging.onrender.com
          VITE_ENABLE_SERVICE_WORKER: true
          VITE_APP_ENV: staging
        run: pnpm run build:staging

      - name: 🏗️ Test Build - Production
        working-directory: ./client2
        env:
          VITE_API_BASE_URL: https://library-server-m6gr.onrender.com
          VITE_AI_SERVICE_URL: https://literati-ai-production.onrender.com
          VITE_ENABLE_SERVICE_WORKER: true
          VITE_APP_ENV: production
        run: pnpm run build:production

      # E2E Testing (if not skipped)
      - name: 🎭 E2E Tests
        if: github.event.inputs.skip_tests != 'true'
        working-directory: ./client2
        run: |
          # Install Playwright
          npx playwright install --with-deps
          # Run E2E tests
          pnpm run test:e2e || echo "⚠️ E2E tests failed"
        continue-on-error: true

      # Deployment Decision Logic
      - name: 🎯 Determine Deployment Strategy
        id: deploy-decision
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            SHOULD_DEPLOY="true"
            TARGET_ENV="${{ github.event.inputs.environment }}"
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            SHOULD_DEPLOY="true"
            TARGET_ENV="production"
          elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
            SHOULD_DEPLOY="true"
            TARGET_ENV="staging"
          else
            SHOULD_DEPLOY="false"
            TARGET_ENV="none"
          fi

          echo "should-deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
          echo "target-env=$TARGET_ENV" >> $GITHUB_OUTPUT
          echo "🎯 Deployment Decision: $SHOULD_DEPLOY to $TARGET_ENV"

  # Staging Deployment
  deploy-staging:
    name: 🚧 Deploy to Staging
    runs-on: ubuntu-latest
    needs: quality-gates
    if: needs.quality-gates.outputs.should-deploy == 'true' && needs.quality-gates.outputs.target-env == 'staging'
    environment: staging

    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
          cache-dependency-path: 'client2/pnpm-lock.yaml'

      - name: 📦 Install Dependencies
        working-directory: ./client2
        run: pnpm install --frozen-lockfile

      - name: 🏗️ Build for Staging
        working-directory: ./client2
        env:
          VITE_API_BASE_URL: https://literati-api-staging.onrender.com
          VITE_AI_SERVICE_URL: https://literati-ai-staging.onrender.com
          VITE_ENABLE_SERVICE_WORKER: true
          VITE_APP_ENV: staging
          VITE_SUPABASE_URL: ${{ secrets.STAGING_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.STAGING_SUPABASE_ANON_KEY }}
        run: pnpm run build:staging

      - name: 🚀 Deploy to Vercel (Staging)
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          working-directory: ./client2
          vercel-args: '--local-config ../vercel.staging.json --prod'
          alias-domains: staging-literati.vercel.app

      - name: 🔍 Staging Health Check
        run: |
          sleep 60
          curl -f https://staging-literati.vercel.app/health || echo "⚠️ Staging health check failed"

  # Production Deployment
  deploy-production:
    name: 🌟 Deploy to Production
    runs-on: ubuntu-latest
    needs: quality-gates
    if: needs.quality-gates.outputs.should-deploy == 'true' && needs.quality-gates.outputs.target-env == 'production'
    environment: production

    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
          cache-dependency-path: 'client2/pnpm-lock.yaml'

      - name: 📦 Install Dependencies
        working-directory: ./client2
        run: pnpm install --frozen-lockfile

      - name: 🏗️ Build for Production
        working-directory: ./client2
        env:
          VITE_API_BASE_URL: https://library-server-m6gr.onrender.com
          VITE_AI_SERVICE_URL: https://literati-ai-production.onrender.com
          VITE_ENABLE_SERVICE_WORKER: true
          VITE_APP_ENV: production
          VITE_SUPABASE_URL: ${{ secrets.PRODUCTION_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.PRODUCTION_SUPABASE_ANON_KEY }}
        run: pnpm run build:production

      - name: 🚀 Deploy to Vercel (Production)
        id: vercel-deploy
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          working-directory: ./client2
          vercel-args: '--prod'

      - name: 🌐 Update Custom Domain Aliases
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        run: |
          echo "🔗 Updating domain aliases to point to latest deployment..."
          echo "📍 Deployment URL: ${{ steps.vercel-deploy.outputs.preview-url }}"

          # Install Vercel CLI
          npm install --global vercel@latest

          # Set alias for main domain
          vercel alias set ${{ steps.vercel-deploy.outputs.preview-url }} literati.pro --token=$VERCEL_TOKEN

          # Set alias for www subdomain
          vercel alias set ${{ steps.vercel-deploy.outputs.preview-url }} www.literati.pro --token=$VERCEL_TOKEN

          echo "✅ Domain aliases updated successfully"
          echo "🌐 literati.pro → ${{ steps.vercel-deploy.outputs.preview-url }}"
          echo "🌐 www.literati.pro → ${{ steps.vercel-deploy.outputs.preview-url }}"

      - name: 🔍 Production Health Check
        run: |
          sleep 60
          curl -f https://library-server-m6gr.onrender.com/health || echo "⚠️ Production health check failed"

      - name: 📊 Post-deployment Performance Check
        run: |
          npm install -g lighthouse
          lighthouse https://your-production-domain.vercel.app \
            --chrome-flags="--headless --no-sandbox" \
            --output=json \
            --output-path=./lighthouse-report.json

          # Extract key metrics
          node -e "
            const report = JSON.parse(require('fs').readFileSync('./lighthouse-report.json'));
            const metrics = report.lhr.audits;
            console.log('🚀 Performance Score:', Math.round(report.lhr.categories.performance.score * 100));
            console.log('⚡ First Contentful Paint:', metrics['first-contentful-paint'].displayValue);
            console.log('🎯 Largest Contentful Paint:', metrics['largest-contentful-paint'].displayValue);
          "

  # Notification
  notify:
    name: 📢 Deployment Notification
    runs-on: ubuntu-latest
    needs: [quality-gates, deploy-staging, deploy-production]
    if: always() && needs.quality-gates.outputs.should-deploy == 'true'

    steps:
      - name: 📢 Success Notification
        if: ${{ (needs.deploy-staging.result == 'success' && needs.quality-gates.outputs.target-env == 'staging') || (needs.deploy-production.result == 'success' && needs.quality-gates.outputs.target-env == 'production') }}
        uses: actions/github-script@v7
        with:
          script: |
            const env = '${{ needs.quality-gates.outputs.target-env }}';
            const domain = env === 'production' ? 'your-production-domain.vercel.app' : 'staging-literati.vercel.app';

            github.rest.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: `✅ **Deployment Successful**\n\n` +
                    `**Environment:** ${env}\n` +
                    `**URL:** https://${domain}\n` +
                    `**Commit:** ${context.sha.substring(0, 7)}\n` +
                    `**Deployed:** ${new Date().toISOString()}\n\n` +
                    `🔗 [View Application](https://${domain})`
            });

      - name: 📢 Failure Notification
        if: ${{ needs.deploy-staging.result == 'failure' || needs.deploy-production.result == 'failure' }}
        uses: actions/github-script@v7
        with:
          script: |
            const env = '${{ needs.quality-gates.outputs.target-env }}';

            github.rest.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: `❌ **Deployment Failed**\n\n` +
                    `**Environment:** ${env}\n` +
                    `**Commit:** ${context.sha.substring(0, 7)}\n` +
                    `**Failed:** ${new Date().toISOString()}\n\n` +
                    `Please check the workflow logs for details.`
            });